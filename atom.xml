<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>感谢每一缕阳光</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-09-19T02:14:54.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Dikea</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Neural Network Methods for Natural Language Processing | 读书笔记</title>
    <link href="http://yoursite.com/2017/09/19/Neural-Network-Methods-for-Natural-Language-Processing-Reading-Notes/"/>
    <id>http://yoursite.com/2017/09/19/Neural-Network-Methods-for-Natural-Language-Processing-Reading-Notes/</id>
    <published>2017-09-19T02:12:28.000Z</published>
    <updated>2017-09-19T02:14:54.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Sentence Representations | 句向量工作</title>
    <link href="http://yoursite.com/2017/09/18/Sentence-Representations/"/>
    <id>http://yoursite.com/2017/09/18/Sentence-Representations/</id>
    <published>2017-09-18T05:47:24.000Z</published>
    <updated>2017-09-18T09:23:40.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>近期，开始进行句向量相关的工作，学习过程记录于此，以便日后查阅。</p></blockquote><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><h3 id="Supervised-Learning-of-Universal-Sentence-Representations-from-Natural-Language-Inference-Data"><a href="#Supervised-Learning-of-Universal-Sentence-Representations-from-Natural-Language-Inference-Data" class="headerlink" title="Supervised Learning of Universal Sentence Representations from Natural Language Inference Data"></a>Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</h3><h4 id="论文主题"><a href="#论文主题" class="headerlink" title="论文主题"></a>论文主题</h4><p>在斯坦福自然语言推断数据集(SNLI)上，使用监督学习的方法训练句向量模型，获得了不错的效果。<br>类似于机器视觉中，在ImageNet上获取的特征可以用到其他图像处理任务中，<br>我们的句向量模型训练好之后，也可以应用到其他的NLP任务中。</p><h4 id="相关资源"><a href="#相关资源" class="headerlink" title="相关资源"></a>相关资源</h4><ul><li><a href="https://arxiv.org/pdf/1705.02364.pdf" target="_blank" rel="external">paper source</a></li><li><a href="https://github.com/facebookresearch/InferSent" target="_blank" rel="external">github source</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;近期，开始进行句向量相关的工作，学习过程记录于此，以便日后查阅。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;References&quot;&gt;&lt;a href=&quot;#References&quot; class=&quot;headerlink&quot; title=&quot;Refer
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>《人性的弱点》 读书与思考</title>
    <link href="http://yoursite.com/2017/09/17/%E4%BA%BA%E6%80%A7%E7%9A%84%E5%BC%B1%E7%82%B9/"/>
    <id>http://yoursite.com/2017/09/17/人性的弱点/</id>
    <published>2017-09-17T03:44:52.000Z</published>
    <updated>2017-09-17T12:00:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>以前，是拒绝看《人性的弱点》这本书的。因为随意翻阅几页之后，草率地认为这又是鸡汤书。<br>直到有一天闲来无聊，在公司的图书架上，翻了翻这本书，才发现，这是本宝贝。</p></blockquote><p>从这本书中，我看到了一些无价的原则，倘若能够习得一二，在为人处世中也会大有裨益。记录在这博客上，以便长期翻阅、警醒。</p><h2 id="让对方成为对话的主导者。"><a href="#让对方成为对话的主导者。" class="headerlink" title="让对方成为对话的主导者。"></a>让对方成为对话的主导者。</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;以前，是拒绝看《人性的弱点》这本书的。因为随意翻阅几页之后，草率地认为这又是鸡汤书。&lt;br&gt;直到有一天闲来无聊，在公司的图书架上，翻了翻这本书，才发现，这是本宝贝。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从这本书中，我看到了一些无价的原则，倘若
      
    
    </summary>
    
    
      <category term="我思" scheme="http://yoursite.com/tags/%E6%88%91%E6%80%9D/"/>
    
  </entry>
  
  <entry>
    <title>学习日记</title>
    <link href="http://yoursite.com/2017/09/11/Learning-Diary/"/>
    <id>http://yoursite.com/2017/09/11/Learning-Diary/</id>
    <published>2017-09-11T09:15:58.000Z</published>
    <updated>2017-09-15T02:51:04.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在这自留地写下所学所思所行，作为青春年华的一面镜子，照亮自己。</p></blockquote><h3 id="2017-9-11"><a href="#2017-9-11" class="headerlink" title="2017-9-11"></a>2017-9-11</h3><h3 id="学"><a href="#学" class="headerlink" title="学"></a>学</h3><p>阅读n-gram文章，<br>统共28页，没有看完，明天继续。<br>观看吴恩达授课视频：深度学习与神经网络，<br>对于神经网络反向传播重要公式的推导过程，<br>还存在些许疑惑。</p><h3 id="思"><a href="#思" class="headerlink" title="思"></a>思</h3><p>今天是研究生开学第一天，<br>见证了两百人的教室，涌入了六百人的壮观场景；<br>也见证了史上最短开学典礼的特殊和莫名兴奋<br>（昨日因下雨，国科大两分钟结束开学典礼）。</p><p>现在开始把精力放在自然语言处理领域，<br>比较重要的是，学习使用深度学习（神经网络）。<br>万事开头难，唯有一步步积累，每日有所长进，<br>才会有阶段性成果。</p><h3 id="行"><a href="#行" class="headerlink" title="行"></a>行</h3><p>上课，<br>自我学习，<br>跑步交友。</p><h3 id="2017-9-15"><a href="#2017-9-15" class="headerlink" title="2017-9-15"></a>2017-9-15</h3><h3 id="学-1"><a href="#学-1" class="headerlink" title="学"></a>学</h3><p>阅读完基础的n-gram部分。<br>阅读书籍<em>Neural networks and deep learning</em>第三章，<br>学习了过拟合和正则化等知识，理解神经网络的工作方式。<br>对自己的提问：</p><ul><li>神经网络为什么是这样的形式？</li><li>如何从数学和逻辑的角度理解BP的公式？</li><li>正则化的操作为什么能够缓解过拟合？</li></ul><h3 id="思-1"><a href="#思-1" class="headerlink" title="思"></a>思</h3><p>学习就是这样，从一无所知到不断碰壁探索直至艰难地步入正轨。</p><h3 id="行-1"><a href="#行-1" class="headerlink" title="行"></a>行</h3><p>多跑步，多交友，多思考。<br>对人真诚，保持微笑，阳光在脸上。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;在这自留地写下所学所思所行，作为青春年华的一面镜子，照亮自己。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;2017-9-11&quot;&gt;&lt;a href=&quot;#2017-9-11&quot; class=&quot;headerlink&quot; title=&quot;2017-9-1
      
    
    </summary>
    
    
      <category term="Learning" scheme="http://yoursite.com/tags/Learning/"/>
    
  </entry>
  
  <entry>
    <title>谈谈我自己（二）</title>
    <link href="http://yoursite.com/2017/07/02/%E8%B0%88%E8%B0%88%E6%88%91%E8%87%AA%E5%B7%B1%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2017/07/02/谈谈我自己（二）/</id>
    <published>2017-07-02T00:38:30.000Z</published>
    <updated>2017-07-02T01:23:44.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>心有猛虎，须细嗅蔷薇</p></blockquote><p>转眼，已经毕业了一个星期了，开始和社会进行近距离亲密接触了。距离上一次总结，时间划过了六个月，一年能够有多少个六个月，一生能有多少个六个月，时光总是最无情的，无论你珍惜不珍惜，它总是走着；时光总是最有情的，无论你愿意不愿意，它总是记录着。</p><h2 id="这半年，我做了什么"><a href="#这半年，我做了什么" class="headerlink" title="这半年，我做了什么"></a>这半年，我做了什么</h2><p><strong>毕业</strong>，完成了毕设，成绩A+，第一次很认真完成的项目。毕业，给人生的前22年划上一个句号<br><strong>旅游</strong>，和欣欣毕业旅游，一路走过：杭州、苏州、上海、南京，很多难忘的风景，铭心的风光<br><strong>工作</strong>，找实习工作，来到了来也，一个期待中的公司与期待中的工作，学习、交友，不亦乐乎</p><h2 id="下半年，打算做什么"><a href="#下半年，打算做什么" class="headerlink" title="下半年，打算做什么"></a>下半年，打算做什么</h2><p><strong>珍惜</strong>，珍惜研一的读书时光，学习心中想学的，学好必须要学的<br><strong>锻炼</strong>，加强游泳、滑板技能，或许，我也是个滑板达人哦<br><strong>交友</strong>，天南海北，学会生活<br><strong>责任</strong>，学会承担责任，虽不为君子，仍需一诺千金</p><h2 id="对自己说的话"><a href="#对自己说的话" class="headerlink" title="对自己说的话"></a>对自己说的话</h2><p>Dikea，不要局限自己，现在的努力，是为了以后的不留遗憾，只要你想，就努力去做。此外，期待自己少一点浮躁，多一些专注。</p><h2 id="附：四年前的只言片语"><a href="#附：四年前的只言片语" class="headerlink" title="附：四年前的只言片语"></a>附：四年前的只言片语</h2><blockquote><p>勇者，就是一个奇迹</p></blockquote><p>勇者，<br>是微笑着面对不幸的人<br>摔倒后<br>毅然跃起<br>含笑前行<br>执着于远方的梦想<br>心中那盏明灯永不灭</p><p>也许，<br>远方是去不了的地方<br>但，<br>那又怎样<br>先让心灵去抵达梦想的彼岸<br>然后，<br>奋斗、拼搏、永不言弃<br>让身再抵达</p><p>也许，<br>我们的存在<br>早已是一个感动鸟语花香的奇迹<br>我已是永不言弃的勇者<br>奋斗，拼搏<br>为了爱我的人<br>为了我爱的人</p><p>勇者，就是一个奇迹</p><p>2013年2月6日</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;心有猛虎，须细嗅蔷薇&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;转眼，已经毕业了一个星期了，开始和社会进行近距离亲密接触了。距离上一次总结，时间划过了六个月，一年能够有多少个六个月，一生能有多少个六个月，时光总是最无情的，无论你珍惜不珍惜，它总是走
      
    
    </summary>
    
    
      <category term="我思" scheme="http://yoursite.com/tags/%E6%88%91%E6%80%9D/"/>
    
  </entry>
  
  <entry>
    <title>The Non-Designer&#39;s Design Book | Reading Notes</title>
    <link href="http://yoursite.com/2017/07/01/The-Non-Designer-s-Design-Book-Reading-Notes/"/>
    <id>http://yoursite.com/2017/07/01/The-Non-Designer-s-Design-Book-Reading-Notes/</id>
    <published>2017-07-01T11:12:05.000Z</published>
    <updated>2017-07-01T11:12:05.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Neural Networks and Deep Learning | 读书笔记</title>
    <link href="http://yoursite.com/2017/07/01/Neural-Networks-and-Deep-Learning-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2017/07/01/Neural-Networks-and-Deep-Learning-读书笔记/</id>
    <published>2017-07-01T08:27:52.000Z</published>
    <updated>2017-09-17T11:23:04.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>前言</strong>：自己对自然语言处理方向比较感兴趣，眼下NLP与深度学习神经网络结合的比较紧密。于是也开始进行神经网络相关知识的学习，近来闲暇的时候，会去阅读一本书：<a href="http://neuralnetworksanddeeplearning.com" target="_blank" rel="external">Neural Networks and Deep Learning</a>。这本书是写给深度学习小白的入门书，读者只要对微积分有些许的了解，外加一些简单的逻辑推理，就可以不费力的阅读。</p><h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><ul><li>What this book is about: 叙述了这本书的写作目的。</li><li>On the exercises and problems: 说明了书中的exercise和problem板块的作用。</li></ul><h2 id="Using-neural-nets-to-recognize-handwritten-digits"><a href="#Using-neural-nets-to-recognize-handwritten-digits" class="headerlink" title="Using neural nets to recognize handwritten digits"></a>Using neural nets to recognize handwritten digits</h2><p>介绍了最为简单的神经网络框架，<br>并使用简洁的代码搭建了一个简单的神经网络。</p><h2 id="How-the-backpropagation-algorithm-works"><a href="#How-the-backpropagation-algorithm-works" class="headerlink" title="How the backpropagation algorithm works"></a>How the backpropagation algorithm works</h2><p>阅读完这一章，记在脑海中的，是BP1~4公式。<br>作者通过多种方式，去解释公式的缘由以及合理性。</p><h2 id="Improving-the-way-neural-networks-learn"><a href="#Improving-the-way-neural-networks-learn" class="headerlink" title="Improving the way neural networks learn"></a>Improving the way neural networks learn</h2><p>之前介绍的是最简单的神经网络，<br>存在很多对其进行优化的地方,所以，<br>这一章将会介绍几种常见的技巧，<br>去提高神经网络学习的质量。</p><h3 id="交叉熵费用函数"><a href="#交叉熵费用函数" class="headerlink" title="交叉熵费用函数"></a>交叉熵费用函数</h3><p><strong>函数作用</strong>：使得神经网络学习速度不会很慢/平缓。<br>文中从原理阐述了该函数存在的合理性。</p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>阐述了什么是过拟合，如果判断是否出现过拟合现象。<br>神经网络训练过程不可避免会陷入过拟合，<br>这种缺陷是无法避免的，只能加以缓解。<br>那么，有哪些办法可以加以改善呢？<br>这就引出了本章的后续部分：<strong>正则化和Dropout</strong>。</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>在费用函数上加一个正则化项（regularization），<br>例如L1范数或者L2范数。<br><strong>正则化目的</strong>：改善过拟合, 提高准确率。<br><strong>如何起作用</strong>：正则化能够通过梯度下降使权重矩阵逐渐变小，<br>如果权重矩阵比较大，根据BP公式可以看出，<br>当输入有了小的变化之后，权重矩阵会收到影响产生变化。<br>而权重矩阵比较小时，这种影响是较小的。<br>因此，正则化的神经网络在通常情况下，<br>能够很好的学习大部分训练数据呈现出来的简单通用的模式，<br>并且很少受少数特殊数据特征的影响。</p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>在神经网络训练过程中，对于每一层，随机选择部分神经元进行训练，<br>剩余神经元暂时隐藏，这种操作即为dropout。<br><strong>如何起作用</strong>：随机选择神经元的操作，使得每次训练时网络的结构都有所不同，<br>这样更加贴近实际应用的随机性，使得训练出来的模型更加能够适应实际应用环境，<br>从而使得模型效果得到提升。</p><h2 id="扩展数据集"><a href="#扩展数据集" class="headerlink" title="扩展数据集"></a>扩展数据集</h2><p>这里指通过一些人为的策略，去扩展训练数据集。<br>文中首先阐述了深度学习的一个规律：<br>通常情况下，训练数据集越大，训练出的模型越好。<br>例如，对于MINST数据集，可以将数字图片旋转较小的一个角度，<br>得到一些新的数字图片，从而扩展了训练数据集，增加了多样性，<br>使得训练出的数字识别神经网络更加有效。<br>当然，对于数据的人为拓展，不能是毫无缘由的，<br>遵循的原则是：扩展的数据，应该能够反应真实世界中数据的变化规律。</p><h2 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h2><p>好的初始化权重能够使得神经网络从训练开始就避免隐藏层神经元学习缓慢，<br>从而提高模型学习速度。对于有些情况，好的权重初始化策略，也能够提升模型的效果。</p><h2 id="超参数的选择"><a href="#超参数的选择" class="headerlink" title="超参数的选择"></a>超参数的选择</h2><p>一个神经网络，通常有诸多超参数需要设置，而在这之前，我们对于好的超参数组合一无所知。<br>所以，在选择超参数的时候，需要遵循一些小的技巧。</p><h2 id="A-visual-proof-that-neural-nets-can-compute-any-function"><a href="#A-visual-proof-that-neural-nets-can-compute-any-function" class="headerlink" title="A visual proof that neural nets can compute any function"></a>A visual proof that neural nets can compute any function</h2><h2 id="Why-are-deep-neural-networks-hard-to-train"><a href="#Why-are-deep-neural-networks-hard-to-train" class="headerlink" title="Why are deep neural networks hard to train?"></a>Why are deep neural networks hard to train?</h2><h2 id="Deep-learning"><a href="#Deep-learning" class="headerlink" title="Deep learning"></a>Deep learning</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;：自己对自然语言处理方向比较感兴趣，眼下NLP与深度学习神经网络结合的比较紧密。于是也开始进行神经网络相关知识的学习，近来闲暇的时候，会去阅读一本书：&lt;a href=&quot;http://neuralnetworksanddeeplearni
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Useful Python Code | 有用的python代码片段</title>
    <link href="http://yoursite.com/2017/07/01/Useful-Python-Code/"/>
    <id>http://yoursite.com/2017/07/01/Useful-Python-Code/</id>
    <published>2017-07-01T05:43:33.000Z</published>
    <updated>2017-07-01T08:26:37.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning in Action | 读书笔记</title>
    <link href="http://yoursite.com/2017/06/10/Machine-Learning-in-Action-Reading-Notes/"/>
    <id>http://yoursite.com/2017/06/10/Machine-Learning-in-Action-Reading-Notes/</id>
    <published>2017-06-10T08:11:39.000Z</published>
    <updated>2017-06-21T08:38:04.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>前言</strong>：最近在阅读《机器学习实战》这本书。为了追求速度，看的是中文的译本。这本书对于基础的机器学习算法讲解的很细致，易于理解。为了方便记忆，特提炼看到的心得，作笔记如下。此笔记只为笔者所作，读者若是觉得逻辑混乱，大可不看。</p><h2 id="第一章-机器学习基础"><a href="#第一章-机器学习基础" class="headerlink" title="第一章 机器学习基础"></a>第一章 机器学习基础</h2><h3 id="本章概要"><a href="#本章概要" class="headerlink" title="本章概要"></a>本章概要</h3><p>讲解了机器学习的概念和关键术语。说明了使用python作为机器学习入门语言的优势。介绍了一些python中常见的机器学习函数库。</p><h2 id="第二章-K近邻算法"><a href="#第二章-K近邻算法" class="headerlink" title="第二章 K近邻算法"></a>第二章 K近邻算法</h2><h3 id="本章概要-1"><a href="#本章概要-1" class="headerlink" title="本章概要"></a>本章概要</h3><p>讲解了K近邻算法的原理，将其应用到改进约会网站的配对效果以及手写数字的分类识别中。</p><h3 id="算法理解"><a href="#算法理解" class="headerlink" title="算法理解"></a>算法理解</h3><p>K近邻算法：存在一个训练数据集合，其中每个数据都有着标签。输入没有标签的测试数据之后，将测试数据的每个特征与训练数据对应的特征进行比较，然后提取训练数据集中最近邻的的前k个分类数据（一般k不大于20）。最后，选择这k个相似数据中出现次数最多的分类，作为新数据的分类。</p><h2 id="第三章-决策树"><a href="#第三章-决策树" class="headerlink" title="第三章 决策树"></a>第三章 决策树</h2><h3 id="本章概要-2"><a href="#本章概要-2" class="headerlink" title="本章概要"></a>本章概要</h3><p>介绍了决策树的主要思想，实现了决策树的代码，并且可视化出了决策树。这里主要介绍的是使用ID3算法生成的决策树，并且使用决策树预测隐形眼镜类型。</p><h3 id="算法理解-1"><a href="#算法理解-1" class="headerlink" title="算法理解"></a>算法理解</h3><p>决策树通过在每个节点的推断分解，逐步缩小数据的所属范围，从而最终得到数据较为准确的分类。在每个节点对数据进行分类的时候，都需要寻找数据最好的分类特征。那么，什么特征称得上最好呢？如果这个特征包含的<strong>信息</strong>越多，根据这个特征的分类肯定是比较合理的。因为包含的信息越多，就越能将大部分数据分成不同的类别。相反，如果包含的信息很少，只能作用于很少的数据，使得分类的效果不好。这里信息的度量方式，使用著名的<strong>信息熵</strong>（香农大神提出的）来表示。</p><blockquote><p>注意：决策树的训练需要耗费大量时间，但使用构建好的决策树去分类问题却很快。为了节省计算时间，可以使用python模块pickle序列化对象，序列化对象可以保存到磁盘，也就是模型可以保存到磁盘，每次需要使用的时候再读取出来。</p></blockquote><h2 id="第四章-基于概率论的分类方法：朴素贝叶斯"><a href="#第四章-基于概率论的分类方法：朴素贝叶斯" class="headerlink" title="第四章 基于概率论的分类方法：朴素贝叶斯"></a>第四章 基于概率论的分类方法：朴素贝叶斯</h2><h3 id="本章概要-3"><a href="#本章概要-3" class="headerlink" title="本章概要"></a>本章概要</h3><p>学习使用概率分布进行分类，学习朴素贝叶斯分类器，并且使用其来分析不同地区的态度。</p><h3 id="算法理解-2"><a href="#算法理解-2" class="headerlink" title="算法理解"></a>算法理解</h3><p>首先，如果不能够从原理理解贝叶斯公式，也可以简单的通过推导得到该公式。我们可以通过该公式，从已有的（训练数据集）概率来推算未知数据所属集合，继而得到分类结果。这一章不仅仅讲了贝叶斯，也描述了一些其他的概念，例如词向量、RSS等等，以及一些编程的技巧，着实学习到了不少东西。</p><h2 id="第五章-Logistic回归"><a href="#第五章-Logistic回归" class="headerlink" title="第五章 Logistic回归"></a>第五章 Logistic回归</h2><h3 id="本章概要-4"><a href="#本章概要-4" class="headerlink" title="本章概要"></a>本章概要</h3><p>这一章是最优化算法的入门，介绍了Logistics回归算法以及sigmoid函数，理论和实践结合，让我第一次明白这个东西有啥用了。我们可以利用sigmoid函数来进行分类，该函数的输入为：Z = W0X0 + W1X1 + … + WnXn,其中系数X0, X1, …, Xn的确定，可以通过最优化算法确定较优的回归系数。本章介绍的最优化算法：梯度上升法，梯度下降法（前者可以用来求解最大值，后者可以用来求解最小值），另外还有优化的随机梯度上升法。</p><h3 id="算法理解-3"><a href="#算法理解-3" class="headerlink" title="算法理解"></a>算法理解</h3><p>有时候，不要纠结于公式，而是要从逻辑、思维的角度去看看该怎么做。因为有时候看公式的推导，百思不得其解，如果从逻辑（这里的逻辑有点类似人的直觉）的角度来看，反而很容易得出正确的结论。两者结合，事半功倍。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;：最近在阅读《机器学习实战》这本书。为了追求速度，看的是中文的译本。这本书对于基础的机器学习算法讲解的很细致，易于理解。为了方便记忆，特提炼看到的心得，作笔记如下。此笔记只为笔者所作，读者若是觉得逻辑混乱，大可不看。&lt;/p&gt;
&lt;h2 i
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>谈谈我自己（一）</title>
    <link href="http://yoursite.com/2017/01/02/%E8%B0%88%E8%B0%88%E6%88%91%E8%87%AA%E5%B7%B1/"/>
    <id>http://yoursite.com/2017/01/02/谈谈我自己/</id>
    <published>2017-01-02T12:04:39.000Z</published>
    <updated>2017-07-02T00:38:02.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>心有猛虎，须细嗅蔷薇</p></blockquote><p>新年的一年开始了，转眼，人生已经度过了22个春夏秋冬。在这旅途中，我需要寻找到自己。<br>顺便总结一下刚刚结束的2016下半年，我做了什么。<br>首先是顺利结束了所有的课程，拿到了驾照，学会了游泳。同时，通过自身的努力，挣了一小笔钱，买了自己梦寐以求的新款苹果电脑。和雅欣一起增进感情，也给了初到北京的姐姐一些力所能及的帮助。<br>这是生活上的，那学习呢？成功保研中科院，开始了坐班实习生活，认识了很不错的老师、学长，大家都很nice，但似乎心中对于新的环境还是有着潜潜的惴惴不安，需要尽快适应环境，找到自己的style。</p><blockquote><p>目前最大的问题</p></blockquote><p>new year，我要挑战自己，直面身上的那些臭习惯、坏毛病，移走面前阻碍前行的几座大山。</p><p><strong>向畏惧说不</strong></p><p>因为有所畏惧，在挑战面前，总会倾向于半途放弃，没有坚持的勇气；<br>因为有所畏惧，在机遇面前，总会给自己一个恰如其分的借口，错失良机；</p><p>2017，我要无所畏惧，慢慢来，会更快。</p><p><strong>向拖延说不</strong></p><p>和大多数人一样，我是一个拖延症早期患者，还可以抢救一下。<br>坚决执行番茄工作法，细化每天的小目标。</p><p><strong>向空白说不</strong></p><p>生活，还有远方。<br>想要未来的日子过的安逸一点，需要眼下的未雨绸缪。最惨的莫过于，时钟滴答之后，留下的只是空白。<br>心中时刻有着准备，未来就不会那么无奈。</p><p><strong>2017，我在这里，和你在一起</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;心有猛虎，须细嗅蔷薇&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;新年的一年开始了，转眼，人生已经度过了22个春夏秋冬。在这旅途中，我需要寻找到自己。&lt;br&gt;顺便总结一下刚刚结束的2016下半年，我做了什么。&lt;br&gt;首先是顺利结束了所有的课程，拿到了
      
    
    </summary>
    
    
      <category term="我思" scheme="http://yoursite.com/tags/%E6%88%91%E6%80%9D/"/>
    
  </entry>
  
</feed>
